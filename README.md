# Niko

**AI-powered CLI: explain code, generate shell commands, use any LLM provider.**

Built in Rust. Works on macOS, Linux, and Windows.

```bash
$ niko cmd "find all files larger than 100MB"
find . -type f -size +100M
Copied to clipboard

$ cat main.rs | niko explain
ðŸ“– 42 lines analyzed â€” completed in 2.1s
## Overview
...
```

---

## Features

- **Three Modes** â€” `cmd`, `explain`, `settings`
- **Dynamic LLM Providers** â€” Any OpenAI-compatible API, Claude, or local Ollama
- **Dynamic Model Selection** â€” Fetches available models from the API, no hardcoded lists
- **RAM-Based Restrictions** â€” Prevents selecting models too large for your hardware
- **Auto-Install Ollama** â€” Installs Ollama automatically if not present
- **Smart Code Chunking** â€” Splits large files at function boundaries with context memory between chunks
- **Automatic Retry** â€” Exponential backoff for transient failures (timeouts, rate limits, 5xx errors)
- **Connection Pooling** â€” Keep-alive HTTP connections for fast sequential LLM calls
- **Command Generation** â€” Natural language â†’ shell commands, auto-copied to clipboard
- **Safety Warnings** â€” Flags dangerous commands before execution
- **Cross-Platform** â€” macOS, Linux (Ubuntu/Debian/etc.), Windows

---

## Install

### macOS / Linux

```bash
curl -fsSL https://raw.githubusercontent.com/rgcsekaraa/niko-cli/main/scripts/install.sh | sh
```

### With Cargo

```bash
cargo install --git https://github.com/rgcsekaraa/niko-cli
```

### From Source

```bash
git clone https://github.com/rgcsekaraa/niko-cli.git
cd niko-cli
make install
```

---

## Quick Start

```bash
# First run â€” interactive setup wizard
niko settings configure
```

This will:
1. Show available providers (Ollama, OpenAI, Claude, DeepSeek, Grok, Groq, Mistral, Together, OpenRouter, or custom)
2. For **Ollama**: auto-install if needed â†’ list local models â†’ show downloadable models filtered by your RAM â†’ let you pick
3. For **API providers**: ask for API key â†’ fetch available models from the API â†’ let you pick
4. Save everything to `~/.niko/config.yaml`

---

## Usage

### `cmd` â€” Generate Shell Commands

```bash
$ niko cmd "find python files modified today"
find . -name "*.py" -mtime 0
Copied to clipboard

$ niko cmd "kill process on port 3000"
$ niko cmd "compress logs folder to tar.gz"
$ niko cmd "git commits from last week"
$ niko cmd "show disk usage by directory"
```

### `explain` â€” Explain Code

```bash
# From a file
niko explain -f src/main.rs

# Pipe code in
cat complex_module.py | niko explain

# Paste interactively (live line counter, Ctrl-D or two empty lines to finish)
niko explain
```

For large files, Niko:
1. **Chunks** code at function/block boundaries (max 200 lines/chunk)
2. **Carries context** â€” each chunk includes overlapping lines and a running summary from previous chunks
3. **Retries** failed LLM calls with exponential backoff (3 attempts, 500ms â†’ 4s delay)
4. **Synthesises** chunk analyses into an overall summary with follow-up questions

### `settings` â€” Configuration

```bash
# Interactive setup wizard
niko settings configure

# Show current config
niko settings show

# Set a value directly
niko settings set openai.api_key sk-xxx
niko settings set openai.model gpt-4o
niko settings set active_provider openai

# Reset to defaults
niko settings init

# Print config path
niko settings path
```

### Override Provider Per-Command

```bash
niko cmd "list files" --provider openai
niko explain -f main.rs --provider claude
```

---

## Reliability & Performance

Niko is designed for production use with reliability and speed:

| Feature | Details |
|---------|---------|
| **Streaming** | Tokens appear immediately as the LLM generates them (all providers) |
| **Retry** | 3 attempts with exponential backoff (500ms â†’ 2s + jitter) |
| **Retryable errors** | Timeouts, connection resets, 429/5xx, rate limits, model loading |
| **Connection pooling** | HTTP keep-alive, 4 idle connections/host, TCP keepalive 30s |
| **Model keep-alive** | Ollama keeps model in VRAM for 30 min (no reload between calls) |
| **Flash attention** | Enabled by default for Ollama (faster on Apple Silicon / GPU) |
| **Adaptive tokens** | `cmd` mode uses 512 max tokens, `explain` uses 4096 â€” less KV cache for short tasks |
| **Adaptive context** | Ollama context window scales with prompt size (4K â†’ 16K) |
| **Empty response guard** | Detects and retries empty/null LLM responses |
| **Truncation detection** | Warns when response hits max_tokens (Claude, OpenAI) |
| **Context memory** | Multi-chunk explanations carry 10-line code overlap for boundary continuity |
| **Structured errors** | Parses API error responses for clear, actionable messages |

---

## Supported Providers

| Provider | Type | How to set up |
|----------|------|---------------|
| **Ollama** | Local (free) | Auto-installed, models downloaded on demand |
| **OpenAI** | API | `niko settings configure` â†’ select OpenAI â†’ enter key |
| **Claude** | API | `niko settings configure` â†’ select Claude â†’ enter key |
| **DeepSeek** | API | `niko settings configure` â†’ select DeepSeek â†’ enter key |
| **Grok** | API | `niko settings configure` â†’ select Grok â†’ enter key |
| **Groq** | API | `niko settings configure` â†’ select Groq â†’ enter key |
| **Mistral** | API | `niko settings configure` â†’ select Mistral â†’ enter key |
| **Together** | API | `niko settings configure` â†’ select Together â†’ enter key |
| **OpenRouter** | API | `niko settings configure` â†’ select OpenRouter â†’ enter key |
| **Custom** | API | `niko settings configure` â†’ choose "Custom" â†’ enter URL + key |

All API providers fetch models dynamically from their `/models` endpoint â€” **nothing is hardcoded**.

### Environment Variables

API keys can also be set via environment variables:

```bash
export OPENAI_API_KEY=sk-xxx
export ANTHROPIC_API_KEY=sk-ant-xxx
export DEEPSEEK_API_KEY=xxx
export GROK_API_KEY=xxx
export GROQ_API_KEY=xxx
export TOGETHER_API_KEY=xxx
export MISTRAL_API_KEY=xxx
export OPENROUTER_API_KEY=xxx
```

---

## RAM-Based Model Restrictions

For local models (Ollama), Niko estimates the maximum model size your system can handle:

| System RAM | Max Model Size | 
|------------|---------------|
| 8 GB | ~4B parameters |
| 16 GB | ~12B parameters |
| 32 GB | ~28B parameters |
| 64 GB | ~60B parameters |

Models exceeding your RAM limit are hidden from the selection list. You can still force-select them with a confirmation prompt.

---

## Config File

All settings are stored in `~/.niko/config.yaml`. The file uses a dynamic structure â€” providers are a map, so you can add as many as you want:

```yaml
active_provider: openai
providers:
  ollama:
    kind: ollama
    base_url: http://127.0.0.1:11434
    model: qwen2.5-coder:7b
  openai:
    kind: openai_compat
    api_key: sk-xxx
    base_url: https://api.openai.com/v1
    model: gpt-4o
  claude:
    kind: anthropic
    api_key: sk-ant-xxx
    model: claude-sonnet-4-20250514
```

---

## Uninstall

```bash
rm $(which niko)
rm -rf ~/.niko
```

## License

MIT
